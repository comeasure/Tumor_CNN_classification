{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score"
      ],
      "metadata": {
        "id": "ofF6RwzPUbJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "d3hEuBGgUhOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/pickle_files/\n",
        "!ls"
      ],
      "metadata": {
        "id": "wIDnekE_Ui_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the project directory in Google Drive\n",
        "project_dir = '/content/drive/MyDrive/Tumor_Classification_Deep_Learning_Alogorithm'"
      ],
      "metadata": {
        "id": "hg7EhKejUmgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "hyperparams = {\n",
        "    'learning_rate': 0.00001,\n",
        "    'batch_size': 64,\n",
        "    'dropout_rate': 0.5,\n",
        "    'l2_reg': 0.001,\n",
        "    'rotation_range': 30,\n",
        "    'width_shift_range': 0.2,\n",
        "    'height_shift_range': 0.2,\n",
        "    'shear_range': 0.2,\n",
        "    'zoom_range': 0.2,\n",
        "    'horizontal_flip': True,\n",
        "    'epochs': 50,\n",
        "    'patience': 5,\n",
        "    'min_lr': 0.00001\n",
        "}"
      ],
      "metadata": {
        "id": "SgffHdcgUpkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the balanced training data\n",
        "with open(\"X_train_balanced.pickle\", \"rb\") as pickle_in:\n",
        "    X_train_balanced = pickle.load(pickle_in)\n",
        "\n",
        "with open(\"Y_train_balanced.pickle\", \"rb\") as pickle_in:\n",
        "    Y_train_balanced = pickle.load(pickle_in)\n",
        "\n",
        "# Normalize the balanced training data\n",
        "X_train_balanced = X_train_balanced / 255.0\n",
        "\n",
        "# Convert Y_train_balanced to numpy array\n",
        "Y_train_balanced = np.array(Y_train_balanced)\n",
        "\n",
        "# Reverse the labels in the balanced training data\n",
        "Y_train_balanced = 1 - Y_train_balanced  # Swap 0 and 1"
      ],
      "metadata": {
        "id": "4bNphX3wU6Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(project_dir):\n",
        "    os.makedirs(project_dir)\n",
        "\n",
        "# Load the original dataset\n",
        "with open(\"X_train.pickle\", \"rb\") as pickle_in:\n",
        "    X_train_orig = pickle.load(pickle_in)\n",
        "\n",
        "with open(\"Y_train.pickle\", \"rb\") as pickle_in:\n",
        "    Y_train_orig = pickle.load(pickle_in)\n",
        "\n",
        "# Normalize the data\n",
        "X_train_orig = X_train_orig / 255.0\n",
        "\n",
        "# Convert Y_train_orig to numpy array\n",
        "Y_train_orig = np.array(Y_train_orig)\n",
        "\n",
        "# Reverse the labels\n",
        "Y_train_orig = 1 - Y_train_orig  # Swap 0 and 1\n",
        "\n",
        "# Split the original data into training and validation sets\n",
        "X_train_split, X_val, Y_train_split, Y_val = train_test_split(X_train_orig, Y_train_orig, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "Y_train_split = np.array(Y_train_split)\n",
        "Y_val = np.array(Y_val)\n",
        "\n",
        "# Check shapes and values\n",
        "print(f\"X_train_split shape: {X_train_split.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"Y_train_split shape: {Y_train_split.shape}\")\n",
        "print(f\"Y_val shape: {Y_val.shape}\")"
      ],
      "metadata": {
        "id": "mgIFElm3U3ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(hyperparams):\n",
        "    # Data augmentation for training data\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=hyperparams['rotation_range'],\n",
        "        width_shift_range=hyperparams['width_shift_range'],\n",
        "        height_shift_range=hyperparams['height_shift_range'],\n",
        "        shear_range=hyperparams['shear_range'],\n",
        "        zoom_range=hyperparams['zoom_range'],\n",
        "        horizontal_flip=hyperparams['horizontal_flip'],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Data augmentation for validation data (only rescaling)\n",
        "    val_datagen = ImageDataGenerator()\n",
        "\n",
        "    # Create the VGG16 base model\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom layers on top of the base model\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(hyperparams['l2_reg'])),\n",
        "        BatchNormalization(),\n",
        "        Dropout(hyperparams['dropout_rate']),\n",
        "        Dense(1, activation='sigmoid', kernel_regularizer=l2(hyperparams['l2_reg']))\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=hyperparams['learning_rate']), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Set up callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=hyperparams['patience'], restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=hyperparams['min_lr'])\n",
        "\n",
        "    class_weights = {0: 1., 1: len(Y_train_balanced) / np.sum(Y_train_balanced)}\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_datagen.flow(X_train_balanced, Y_train_balanced, batch_size=hyperparams['batch_size']),\n",
        "        validation_data=val_datagen.flow(X_val, Y_val),\n",
        "        epochs=hyperparams['epochs'],\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        steps_per_epoch=len(X_train_balanced) // hyperparams['batch_size'],\n",
        "        validation_steps=len(X_val) // hyperparams['batch_size'],\n",
        "        class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, Y_val)\n",
        "    print(f\"Validation Loss: {val_loss}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "    return model, history, val_loss, val_accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1TRsjn66UTey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giMYEobjUC8n"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(hyperparams):\n",
        "    # Data augmentation for training data\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=hyperparams['rotation_range'],\n",
        "        width_shift_range=hyperparams['width_shift_range'],\n",
        "        height_shift_range=hyperparams['height_shift_range'],\n",
        "        shear_range=hyperparams['shear_range'],\n",
        "        zoom_range=hyperparams['zoom_range'],\n",
        "        horizontal_flip=hyperparams['horizontal_flip'],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Data augmentation for validation data (only rescaling)\n",
        "    val_datagen = ImageDataGenerator()\n",
        "\n",
        "    # Create the VGG16 base model\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom layers on top of the base model\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(hyperparams['l2_reg'])),\n",
        "        BatchNormalization(),\n",
        "        Dropout(hyperparams['dropout_rate']),\n",
        "        Dense(1, activation='sigmoid', kernel_regularizer=l2(hyperparams['l2_reg']))\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=hyperparams['learning_rate']), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Set up callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=hyperparams['patience'], restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=hyperparams['min_lr'])\n",
        "\n",
        "    class_weights = {0: 1., 1: len(Y_train_balanced) / np.sum(Y_train_balanced)}\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_datagen.flow(X_train_balanced, Y_train_balanced, batch_size=hyperparams['batch_size']),\n",
        "        validation_data=val_datagen.flow(X_val, Y_val),\n",
        "        epochs=hyperparams['epochs'],\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        steps_per_epoch=len(X_train_balanced) // hyperparams['batch_size'],\n",
        "        validation_steps=len(X_val) // hyperparams['batch_size'],\n",
        "        class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, Y_val)\n",
        "    print(f\"Validation Loss: {val_loss}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "    return model, history, val_loss, val_accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate the model with initial hyperparameters\n",
        "model, history, val_loss, val_accuracy = train_and_evaluate_model(hyperparams)"
      ],
      "metadata": {
        "id": "tZ6BZVlWUP4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to Google Drive\n",
        "model.save(os.path.join(project_dir, 'VGG16_model_class_weights.h5'))\n",
        "\n",
        "# Save the model weights to Google Drive\n",
        "model.save_weights(os.path.join(project_dir, 'VGG16_weights_class_weights.h5'))"
      ],
      "metadata": {
        "id": "zcCM1FWkVC5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZESwgYDHVLuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions\n",
        "Y_pred = model.predict(X_val)\n",
        "Y_pred_classes = (Y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(Y_val, Y_pred_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_val, Y_pred_classes, target_names=['Negative', 'Positive']))\n"
      ],
      "metadata": {
        "id": "-kfspMgyVNaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(Y_val, Y_pred)\n",
        "average_precision = average_precision_score(Y_val, Y_pred)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='purple', lw=2, label='Precision-Recall curve (area = %0.2f)' % average_precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "88Ntst9CVS-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# ROC curve\n",
        "fpr, tpr, _ = roc_curve(Y_val, Y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tYp_SeNjVPY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}